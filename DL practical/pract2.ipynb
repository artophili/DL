{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4fb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5000], Loss: 0.7037\n",
      "Epoch [500/5000], Loss: 0.0016\n",
      "Epoch [1000/5000], Loss: 0.0005\n",
      "Epoch [1500/5000], Loss: 0.0003\n",
      "Epoch [2000/5000], Loss: 0.0002\n",
      "Epoch [2500/5000], Loss: 0.0001\n",
      "Epoch [3000/5000], Loss: 0.0001\n",
      "Epoch [3500/5000], Loss: 0.0000\n",
      "Epoch [4000/5000], Loss: 0.0000\n",
      "Epoch [4500/5000], Loss: 0.0000\n",
      "\n",
      "Predictions:\n",
      " tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Aim: Solve the XOR problem using a deep feedforward network and\n",
    "implement gradient-based learning.\n",
    "\n",
    "XOR problem: A function that outputs 1 when one of the inputs is 1 and other is 0\n",
    "XOR is a non-linearly separable problem means a simple linear model can't solve it.\n",
    "\n",
    "Deep FeedForward network:\n",
    "A feedForward network is a type of AI network where the data flows in one direction\n",
    "It consists of an input layer, one or more hidden layers and output layer.\n",
    "We need to use non-linear activation function like sigmoid or ReLU in hidden layer to capture the non-linearity\n",
    "\n",
    "Gradient based learning (Backpropagation):\n",
    "Goal is to minimize the loss function using gradient descent to adjust weights and biases.\n",
    "Backpropagation computes gradients of the loss with respect to weights using the chain\n",
    "rule and updates them iteratively.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn #Helps in creating neural network layers\n",
    "import torch.optim as optim #Optimizers to improve the model\n",
    "\n",
    "# Define the XOR dataset\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "#X : All possible combinations of 2 bits\n",
    "#Y : XOR output\n",
    "\n",
    "# Define the feedforward neural network\n",
    "class XORNet(nn.Module): #nn.Module: Base class for all neural network\n",
    "    def __init__(self):\n",
    "        super(XORNet, self).__init__() #XORNet is a custome network class.\n",
    "        self.hidden = nn.Linear(2, 4) # Hidden layer with 4 neurons\n",
    "        #Takes two inputs X1, X2 and sends to 4 neurons\n",
    "        self.output = nn.Linear(4, 1) # Output layer\n",
    "        #Takes 4 inputs from hidden layer and gives 1 output\n",
    "    def forward(self, x): #How input data flows through the network\n",
    "        x = torch.sigmoid(self.hidden(x)) # Activation function\n",
    "        #sigmoid : Converts output to a number between 0 and 1.\n",
    "        x = torch.sigmoid(self.output(x)) # Activation function\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = XORNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy Loss as the o/p is binary\n",
    "#It measures the error between predicted probability and actual label.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1) \n",
    "# Adam optimizer : Adjusts learning rates automatically\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() #clear old gradients\n",
    "    outputs = model(X) #pass inputs through network to get predicted o/p\n",
    "    loss = criterion(outputs, Y) #Calculate the loss (how wrong the model is).\n",
    "    loss.backward() #Compute gradients (chain rule applied automatically).\n",
    "    optimizer.step() #Adjust model weights based on gradients.\n",
    " \n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
    "        #Printing loss every 5000 epochs\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "    predicted_labels = (predictions > 0.5).float()\n",
    "    #If prediction > 0.5, consider it 1, otherwise 0 it is called as thresholding\n",
    "    print(\"\\nPredictions:\\n\", predicted_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
