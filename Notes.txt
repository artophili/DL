World is currently working to store data on DNA. But it is costly. It takes $7000 to store 2 MB of data and $2000 to read that data again.
What is deep learning ?
Geoffrey Hinton is god father of deep learning.
Here we create a artificial structure called as artificial neural net where we have nodes or neurons. Some neurons are for input values and there will be a output value. In between input and output, there are hidden layers

Part 1 - Artificial neural networks
Objective:
The intuition of ANN
The Neuron : 
A neuron is a basic unit of neural system in human.
A neuron have body consist of a neuron, Dendrites, and a Axon
Dendrites are the receivers of signal and dedrites are carrier of the signal.
There is a gap between dendrites and the axon is called syanpses

A neuron will get some input values and it will give a output value just like a human brain
#Human brain is placed inside a black box i.e It can see, smell anything but it receives impulses from sensory organs (input values) and produce a output value (output).

Input variables are independent variable (It just a observation i.e One row in a database)
We need to standardize the input variables (Mean as zero and variance as one) or need to normalize.

The output value can be
Continuous value
Binary value
Categorical value

#The input value (single row) which we are putting in will produce an ouput for the same row's observation
 
syanpses : They are the weighted signals. Here weights are crucial because that's how the neural network learns.By adjusting the weights, network decides what signal is important and which is not.Which signal should pass 
along and which signal does not contribute in the output.

Inside the neuron : Once the neuron receives the input with the associated weights, it calculates the weighted sum of all the inputs.
summation(WiXi). Then it applies the activation function to the weighted sum and passes that further.
-----------------------------------------------------------------------------------
The activation function :
The predominant 4 types of function :
1. Threshold function: f(x) = 1 if x>= 1 or 0 if x<0
2. Sigmoid function : f(x) = 1/(1+e^-x)
3. Rectifier : f(x) = max(x,0)
4. Hyperbolic Tangent: f(x) = (1-e^-2x)/(1+e^-2x)

How do Neural network works ?

How do neural networks learn ?
There are two approaches for learning
1. Hard coded
2. Neural network : 
Firstly we have the input layers which is made up of only one observation from the DB.
Then we pass on the input values to neuron through syanpses. Here syanpses are the weights
Then we calculate the weigthed sum of all the input variables then pass that through a activation funcion.
Then the output of that function is passed to output layer where it will predict the output as y'
We have the actual output as y
Then we calculate the cost function. There are various cost function available. Cost funtion tells us about the 
error between the predicted output vs actual output.
For example; cost function = 1/2(y'-y)^2
Then we will feed back the cost to the network according to which the weights are adjusted and again the whole procedure repeats till we have a 
minimized cost function
##Goal : To adjust the weights so that the cost function is minimized.

One epoch : When we go through a whole dataset and we train the network on all the rows.

Gradient descent:
This decides how the weights are adjusted
There are multiple approaches to minimize cost function by adjusting the weights.
1. Brute Force approach:
    Here we take all the possible weights and look them and which is better
Curse of dimensionality : 
Let's take an example
Let us consider an neural network with 4 input layers and one hidden layes with 5 neurons
To use brute force approach, we need to consider all the possible weights adjustments to decide which one is better
As per the calculations,
Initially there will be 25 weights to be considered -> (4 * 5) + 5 (from hidden layers)
To brute force all the 25 weights,
If we have 1000 combinations to test through all the 25 weights
It will be calculates as
1000 * 1000 * .....*1000 = 1000^25 = 10^75 combinations of weights
To run this computation, even the world's fastest computer is not sufficient
Because,
The world's fastest computer is Sunway TaihuLight
Which have a speed of 93 PFLOPS that means it can do 93*10^15 peta floating operation per second

Whereas the average computer performs several gigaflops only.

Let's assume it hypothetically that it can test one combination of the neural network from all the 10^75 combinations available 
in one flop. That is actually not possible because we need multiple flops to do it.

Still if we assume like this the also it will require
10^75/(93*10^15) seconds to brute force all the weights in a simple network.
If we calculate above seconds it sums up to
10^75/(93*10^15) = 1.08 * 10^58 seconds = 3.42*10^50 years.

The fact that this above number is so much longer than our universe has existed.
Our universe is 13.8 billion years old.

We need to find a fastest way to optimize the weights
For that we will use Gradient Descent, first we will plot the cost function and decide a random point on the plot and 
we will calculate the slope at that point that is we calculate the gradient at that point and decide whether to go to the right or left acc to slope
If slope is negative, then we will go right and if the slope is positive then we will go left
After repeating these steps, we will reach to the minima of the function where the cost is minimum. That combination will be the best combination of weights.

Stochastiic Descent:
Gradient descent function needs a convex function.
For non convex function with multiple minimas we need to use stochastic function.
In stochastic, we take one row at a time and proecess it trhough the network and optimize the cost function and update the weights for that row only.

Back propagation :
First we forward the inputs to with the weight along the hidden layers. Then we calculate the error between predicted and actual value and 
feed back the error along the network again to adjust the weight to minimize the error.

Steps to train the ANN with stochastic Gradient Descent:
1. Randomly initialise the weights to small numbers close to 0. Not 0.
2. Input the first observation of your dataset in the input layer, each feature in one input node.
3. Forward-Propagation from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights. Propagate the activations until getting the predicted result y'.
4. Compare the predicted result to actual result. Measure the generated error.
5. Back propagation: From right to left, the error is back propagated. Update the weights according to how much they are responsible for the error. The learning rate decides by how much we update the weights.
6. Repeat steps 1 to 5 and update the weights after each observation (Reinforcement learning) or Repeat steps 1 to 5 but update the weights only after a batch of observations (Batch learning)

How to build an ANN
How to predict outcome of single observation
